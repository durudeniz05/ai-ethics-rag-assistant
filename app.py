# -*- coding: utf-8 -*-

# =================================================================================
# 5. ADIM: STREAMLIT WEB UYGULAMASI (Final S羹r羹m - Final Syntax Fix 3)
# =================================================================================

import streamlit as st
import os
import glob
import tempfile
import textwrap
import traceback # Hata takibi i癟in

# RAG Bileenleri - Import Block
# ===========================================
try:
    import google.generativeai as genai
    from google.generativeai.errors import APIError # Corrected APIError import path
    # print("--- google.generativeai and APIError imported successfully ---")
except ImportError as e:
    st.error(f"Kritik Import Hatas覺: google.generativeai veya APIError y羹klenemedi. {repr(e)}")
    st.stop()
except Exception as e:
    st.error(f"Kritik Balang覺癟 Hatas覺 (google import): {repr(e)}")
    st.stop()
# ===========================================

# Other imports (Corrected try-except structure)
# ===========================================
try: # Try block for chromadb
    from chromadb import Client, Settings
    from chromadb.api.models.Collection import Collection
    # print("--- chromadb imported successfully ---")
except ImportError as e: # Except block aligned with try
    print(f"!!! FAILED to import chromadb:")
    print(repr(e))
    st.error(f"Critical Import Error: Failed to load chromadb. Details: {repr(e)}")
    st.stop()

try: # Try block for langchain_google_genai
    from langchain_google_genai import GoogleGenerativeAIEmbeddings
    # print("--- langchain_google_genai imported successfully ---")
except ImportError as e: # Except block aligned with try
    print(f"!!! FAILED to import langchain_google_genai:")
    print(repr(e))
    st.error(f"Critical Import Error: Failed to load langchain_google_genai. Details: {repr(e)}")
    st.stop()

try: # Try block for langchain_text_splitters
    from langchain_text_splitters import RecursiveCharacterTextSplitter
    # print("--- langchain_text_splitters imported successfully ---")
except ImportError as e: # Except block aligned with try
    print(f"!!! FAILED to import langchain_text_splitters:")
    print(repr(e))
    # ===========================================
    # SYNTAX ERROR FIX HERE
    st.error(f"Critical Import Error: Failed to load langchain_text_splitters. Details: {repr(e)}") # Added ')'
    # ===========================================
    st.stop()

try: # Try block for langchain_community
    from langchain_community.document_loaders import PyPDFLoader
    # print("--- langchain_community.document_loaders imported successfully ---")
except ImportError as e: # Except block aligned with try
    print(f"!!! FAILED to import langchain_community.document_loaders:")
    print(repr(e))
    st.error(f"Critical Import Error: Failed to load langchain_community.document_loaders. Details: {repr(e)}")
    st.stop()
# ===========================================


# --- 1. API Key ---
try:
    GEMINI_API_KEY = st.secrets["GEMINI_API_KEY"]
except KeyError: st.error("HATA: Streamlit Secrets'ta 'GEMINI_API_KEY' bulunamad覺."); st.stop()
except Exception as e: st.error(f"Secrets okunurken HATA: {e}"); st.stop()

# --- 2. Setup Components (Cache Enabled) ---
@st.cache_resource # Cache etkin
def setup_rag_components():
    """T羹m RAG bileenlerini balat覺r ve cache'ler."""
    print("--- DEBUG: setup_rag_components ALITIRILIYOR (cache ile)... ---")
    llm, embedding_function, text_splitter, collection = None, None, None, None # Initialize
    try: # Start main try block
        genai.configure(api_key=GEMINI_API_KEY)
        embedding_model_name = "models/text-embedding-004"
        embedding_function = GoogleGenerativeAIEmbeddings(
            model=embedding_model_name,
            google_api_key=GEMINI_API_KEY
        )
        text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=100)
        # ChromaDB (In-Memory)
        try:
            import chromadb
            chroma_client = chromadb.Client()
            collection_name = "ai_ethics_manual_collection"
            try: chroma_client.delete_collection(name=collection_name)
            except: pass
            collection = chroma_client.get_or_create_collection(name=collection_name)
        except Exception as chroma_e:
            print(f"!!! FAILED during ChromaDB initialization: {chroma_e} !!!")
            print(traceback.format_exc())
            st.error(f"ChromaDB balat覺lamad覺: {chroma_e}")
            raise chroma_e # Re-raise
        llm = genai.GenerativeModel('gemini-1.5-flash')
        print("--- DEBUG: setup_rag_components BAARIYLA TAMAMLANDI (cache ile) ---")
        return llm, embedding_function, text_splitter, collection
    # Aligned except block
    except Exception as e:
        print(f"!!! setup_rag_components i癟inde HATA (cache ile): {e} !!!")
        print(traceback.format_exc())
        st.error(f"Uygulama bileenleri balat覺l覺rken bir hata olutu. Detaylar loglarda. Hata: {e}")
        raise e

# --- 3. Veri 襤leme Fonksiyonu ---
def index_documents(uploaded_files, collection, text_splitter, embedding_function):
    """Y羹klenen dosyalar覺 iler ve Vekt繹r Veritaban覺'na kaydeder."""
    chunked_texts, chunk_metadatas, processed_files_count = [], [], 0
    with tempfile.TemporaryDirectory() as temp_dir:
        for uploaded_file in uploaded_files:
            file_processed = False; temp_file_path = os.path.join(temp_dir, uploaded_file.name)
            try:
                with open(temp_file_path, "wb") as f: f.write(uploaded_file.getbuffer())
                loader = PyPDFLoader(temp_file_path); documents = loader.load()
                if not documents: st.warning(f"'{uploaded_file.name}' i癟erik okunamad覺."); continue
                chunks = text_splitter.split_documents(documents)
                if not chunks: st.warning(f"'{uploaded_file.name}' par癟alara ayr覺lamad覺."); continue
                for chunk in chunks:
                    chunked_texts.append(chunk.page_content)
                    metadata = {"source": uploaded_file.name}
                    if hasattr(chunk, 'metadata') and 'page' in chunk.metadata: metadata['page'] = chunk.metadata['page']
                    chunk_metadatas.append(metadata)
                file_processed = True
            except Exception as e: st.error(f"'{uploaded_file.name}' ilenirken hata: {e}"); st.error(traceback.format_exc())
            finally:
                if file_processed: processed_files_count += 1
                if os.path.exists(temp_file_path):
                    try: os.remove(temp_file_path)
                    except OSError as e: pass
    if not chunked_texts: st.error("Y羹klenen ge癟erli PDF dosyalar覺ndan metin 癟覺kar覺lamad覺."); return 0, 0
    try:
        with st.spinner(f"{len(chunked_texts)} par癟a vekt繹re 癟evriliyor..."): embeddings = embedding_function.embed_documents(chunked_texts)
    except Exception as e: st.error(f"Embedding Hatas覺: Vekt繹r oluturulamad覺. Detay: {e}"); return processed_files_count, 0
    ids = [f"doc_{i}" for i in range(len(chunked_texts))]
    try:
        with st.spinner("Vekt繹r veritaban覺na ekleniyor..."): collection.add(documents=chunked_texts, embeddings=embeddings, metadatas=chunk_metadatas, ids=ids)
        return processed_files_count, len(chunked_texts)
    except Exception as e: st.error(f"Vekt繹r DB ekleme hatas覺: {e}"); st.error(traceback.format_exc()); return processed_files_count, 0

# --- 4. RAG Sorgulama Fonksiyonu ---
def ask_rag_assistant(question, llm, collection, embedding_function):
    """RAG sorgusunu 癟al覺t覺r覺r ve cevab覺 d繹nd羹r羹r."""
    global APIError
    try:
        with st.spinner("Sorunuz analiz ediliyor..."): query_vector = embedding_function.embed_query(question)
        with st.spinner("襤lgili dok羹manlar aran覺yor..."): results = collection.query(query_embeddings=[query_vector], n_results=3, include=['metadatas', 'documents'])
        if not results or not results.get('ids') or not results['ids'][0]: return "Veritaban覺nda sorunuzla ilgili bilgi bulunamad覺."
        retrieved_chunks = results['documents'][0]; retrieved_metadatas = results['metadatas'][0]
        context = "\n---\n".join(retrieved_chunks)
        system_prompt = ("Sen bir Yapay Zeka Etii ve Uyum asistan覺s覺n. Yaln覺zca salanan balamdaki bilgilere dayanarak yan覺tla. Eer balamda bilgi yoksa 'Elimdeki dok羹manlarda bu konuyla ilgili spesifik bilgi bulunmamaktad覺r.' diye cevap ver. Cevab覺n覺 k覺sa ve 繹z tut. Cevab覺n sonunda, kullan覺lan kayna覺 '[Kaynak: Dosya Ad覺, Sayfa X]' format覺nda belirt.")
        full_prompt = f"{system_prompt}\n\nBalam:\n{context}\n\nSoru: {question}\n\nCevap:"
        with st.spinner("Cevap oluturuluyor..."): response = llm.generate_content(full_prompt)
        source_info = []
        if retrieved_metadatas:
            for meta in retrieved_metadatas:
                if meta: source, page = meta.get('source', '?'), meta.get('page', None); source_info.append(f"{source}, Sayfa {page + 1}" if page is not None else source)
        unique_source_info = sorted(list(set(source_info)))
        try: final_answer = response.text
        except ValueError as e:
             final_answer = f"Model uygun cevap 羹retemedi. Detay: {repr(e)}"
             try:
                 if hasattr(response, 'prompt_feedback') and response.prompt_feedback:
                     final_answer += f" (Sebep: {response.prompt_feedback})"
             except AttributeError: pass
        if unique_source_info and not any(src.split(',')[0] in final_answer for src in unique_source_info): final_answer += f" [Kaynak: {'; '.join(unique_source_info)}]"
        return final_answer
    except APIError as e: return f"API HATA: Gemini servisine eriim salanamad覺. Detay: {repr(e)}"
    except Exception as e: st.error(f"RAG Sorgulama Hatas覺: {e}"); st.error(traceback.format_exc()); return f"GENEL HATA: RAG sorgusu ilenirken bir sorun olutu."

# =================================================================================
# 5. STREAMLIT ANA FONKS襤YON
# =================================================================================

def main():
    st.set_page_config(page_title="AI Ethics RAG Assistant", layout="wide")
    st.title(" AI Ethics & Compliance RAG Assistant")
    st.markdown("Yapay Zeka Etik ve Uyum Dok羹manlar覺na Dayal覺 Soru-Cevap Asistan覺")
    st.caption("Not: Bu uygulama Google Gemini ve ChromaDB kullanmaktad覺r.")

    # Bileenleri y羹kle (Cache ile)
    try:
        llm, embedding_function, text_splitter, collection = setup_rag_components()
    except Exception as e:
        st.stop() # Error already shown/logged in setup_rag_components

    # Bileenlerin baar覺yla y羹klenip y羹klenmediini kontrol et
    if not llm or not embedding_function or not text_splitter or not collection:
         st.error("Bileenlerden biri veya birka癟覺 y羹klenemedi. Loglar覺 kontrol edin.")
         st.stop()

    # Sidebar
    with st.sidebar:
        st.header("1. Dok羹man Y羹kleme (PDF)")
        uploaded_files = st.file_uploader("AI Etik ve Uyum PDF'lerini y羹kleyin", type="pdf", accept_multiple_files=True, key="file_uploader")

        if st.button("Dok羹manlar覺 襤le ve Kaydet"):
            if uploaded_files:
                try:
                    existing_ids = collection.get(include=[])['ids']
                    if existing_ids: collection.delete(ids=existing_ids); st.info("Mevcut veritaban覺 temizlendi.")
                except Exception as e: pass
                processed_count, chunk_count = index_documents(uploaded_files, collection, text_splitter, embedding_function)
                if chunk_count > 0:
                    st.success(f"Baar覺yla {processed_count}/{len(uploaded_files)} dosya ilendi ve {chunk_count} par癟a kaydedildi.")
                st.rerun()
            else:
                st.warning("L羹tfen ilem yapmak i癟in bir PDF dosyas覺 y羹kleyin.")

        # Mevcut Kay覺t Say覺s覺
        try:
            doc_count
