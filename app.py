# -*- coding: utf-8 -*-

# =================================================================================
# 5. ADIM: STREAMLIT WEB UYGULAMASI (Final S羹r羹m - APIError G羹ncel Versiyona Uyarland覺)
# =================================================================================

import streamlit as st
import os
import glob
import tempfile
import textwrap
import traceback # Hata takibi i癟in

# RAG Bileenleri - Import Block
# ===========================================
try:
    import google.generativeai as genai
    # DZELTME: G羹ncel google-generativeai (0.7.2) paketi i癟in doru import ekli
    from google.generativeai import APIError 
    
    print("--- google.generativeai and APIError imported successfully ---")
except ImportError as e: 
    st.error(f"Kritik Import Hatas覺: google.generativeai veya APIError y羹klenemedi. {repr(e)}"); st.stop()
except Exception as e: 
    st.error(f"Kritik Balang覺癟 Hatas覺 (google import): {repr(e)}"); st.stop()
# ===========================================

# Other imports
try:
    from chromadb import Client, Settings
    from chromadb.api.models.Collection import Collection
except ImportError as e: st.error(f"Kritik Import Hatas覺: chromadb y羹klenemedi. {repr(e)}"); st.stop()
try:
    from langchain_google_genai import GoogleGenerativeAIEmbeddings
except ImportError as e: st.error(f"Kritik Import Hatas覺: langchain_google_genai y羹klenemedi. {repr(e)}"); st.stop()
try:
    from langchain_text_splitters import RecursiveCharacterTextSplitter
except ImportError as e: st.error(f"Kritik Import Hatas覺: langchain_text_splitters y羹klenemedi. {repr(e)}"); st.stop()
try:
    from langchain_community.document_loaders import PyPDFLoader
except ImportError as e: st.error(f"Kritik Import Hatas覺: langchain_community.document_loaders y羹klenemedi. {repr(e)}"); st.stop()


# --- 1. API Key ---
try:
    GEMINI_API_KEY = st.secrets["GEMINI_API_KEY"]
except KeyError: st.error("HATA: Streamlit Secrets'ta 'GEMINI_API_KEY' bulunamad覺."); st.stop()
except Exception as e: st.error(f"Secrets okunurken HATA: {e}"); st.stop()

# --- 2. Setup Components (Cache Enabled) ---
@st.cache_resource 
def setup_rag_components():
    """T羹m RAG bileenlerini balat覺r ve cache'ler."""
    print("--- DEBUG: setup_rag_components ALITIRILIYOR (cache ile)... ---")
    llm, embedding_function, text_splitter, collection = None, None, None, None 

    # --- Start Main Try Block ---
    try:
        # Configure Gemini
        genai.configure(api_key=GEMINI_API_KEY)

        # Embedding Model
        embedding_model_name = "models/text-embedding-004"
        embedding_function = GoogleGenerativeAIEmbeddings(
            model=embedding_model_name,
            google_api_key=GEMINI_API_KEY
        )

        # Text Splitter
        text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=100)

        # ChromaDB (In-Memory)
        try:
            import chromadb
            chroma_client = chromadb.Client() 
            collection_name = "ai_ethics_manual_collection"
            try: chroma_client.delete_collection(name=collection_name)
            except: pass 
            collection = chroma_client.get_or_create_collection(name=collection_name)
        except Exception as chroma_e:
            print(f"!!! FAILED during ChromaDB initialization: {chroma_e} !!!")
            print(traceback.format_exc())
            st.error(f"ChromaDB balat覺lamad覺: {chroma_e}")
            raise chroma_e 
            
        # LLM Model
        llm = genai.GenerativeModel('gemini-1.5-flash')

        print("--- DEBUG: setup_rag_components BAARIYLA TAMAMLANDI (cache ile) ---")
        return llm, embedding_function, text_splitter, collection

    # --- End Main Try Block ---
    except Exception as e:
        print(f"!!! setup_rag_components i癟inde HATA (cache ile): {e} !!!")
        print(traceback.format_exc()) 
        st.error(f"Uygulama bileenleri balat覺l覺rken bir hata olutu. Detaylar loglarda. Hata: {e}")
        raise e

# --- 3. Veri 襤leme Fonksiyonu ---
def index_documents(uploaded_files, collection, text_splitter, embedding_function):
    """Y羹klenen dosyalar覺 iler ve Vekt繹r Veritaban覺'na kaydeder."""
    chunked_texts, chunk_metadatas, processed_files_count = [], [], 0
    with tempfile.TemporaryDirectory() as temp_dir:
        for uploaded_file in uploaded_files:
            file_processed = False; temp_file_path = os.path.join(temp_dir, uploaded_file.name)
            try:
                with open(temp_file_path, "wb") as f: f.write(uploaded_file.getbuffer())
                loader = PyPDFLoader(temp_file_path); documents = loader.load()
                if not documents: st.warning(f"'{uploaded_file.name}' i癟erik okunamad覺."); continue
                chunks = text_splitter.split_documents(documents)
                if not chunks: st.warning(f"'{uploaded_file.name}' par癟alara ayr覺lamad覺."); continue
                for chunk in chunks:
                    chunked_texts.append(chunk.page_content)
                    metadata = {"source": uploaded_file.name}
                    if hasattr(chunk, 'metadata') and 'page' in chunk.metadata: 
                        metadata['page'] = chunk.metadata['page']
                    chunk_metadatas.append(metadata)
                file_processed = True
            except Exception as e: st.error(f"'{uploaded_file.name}' ilenirken hata: {e}"); st.error(traceback.format_exc())
            finally:
                if file_processed: processed_files_count += 1
                if os.path.exists(temp_file_path):
                    try: os.remove(temp_file_path)
                    except OSError as e: pass
                    
    if not chunked_texts: st.error("Y羹klenen ge癟erli PDF dosyalar覺ndan metin 癟覺kar覺lamad覺."); return 0, 0
    
    try:
        with st.spinner(f"{len(chunked_texts)} par癟a vekt繹re 癟evriliyor..."): 
            embeddings = embedding_function.embed_documents(chunked_texts)
    except Exception as e: st.error(f"Embedding Hatas覺: Vekt繹r oluturulamad覺. Detay: {e}"); return processed_files_count, 0
    
    ids = [f"doc_{i}" for i in range(len(chunked_texts))]
    
    try:
        with st.spinner("Vekt繹r veritaban覺na ekleniyor..."): 
            collection.add(documents=chunked_texts, embeddings=embeddings, metadatas=chunk_metadatas, ids=ids)
        return processed_files_count, len(chunked_texts)
    except Exception as e: 
        st.error(f"Vekt繹r DB ekleme hatas覺: {e}"); st.error(traceback.format_exc()); 
        return processed_files_count, 0

# --- 4. RAG Sorgulama Fonksiyonu ---
def ask_rag_assistant(question, llm, collection, embedding_function):
    """RAG sorgusunu 癟al覺t覺r覺r ve cevab覺 d繹nd羹r羹r."""
    try:
        # 1. Sorguyu vekt繹re 癟evir
        with st.spinner("Sorunuz analiz ediliyor..."): 
            query_vector = embedding_function.embed_query(question)
            
        # 2. Vekt繹r veritaban覺nda en yak覺n par癟alar覺 bul
        with st.spinner("襤lgili dok羹manlar aran覺yor..."): 
            results = collection.query(query_embeddings=[query_vector], n_results=3, include=['metadatas', 'documents'])
            
        # Sonu癟 kontrol羹
        if not results or not results.get('ids') or not results['ids'][0]: 
            return "Veritaban覺nda sorunuzla ilgili bilgi bulunamad覺."
        
        # 3. Balam覺 olutur (Noktal覺 virg羹l kald覺r覺ld覺, girintisi d羹zeltildi)
        retrieved_chunks = results['documents'][0]
        retrieved_metadatas = results['metadatas'][0]
        context = "\n---\n".join(retrieved_chunks)
        
        # 4. Prompt'u olutur ve LLM'e g繹nder
        system_prompt = (
            "Sen bir Yapay Zeka Etii ve Uyum asistan覺s覺n. Yaln覺zca salanan balamdaki bilgilere dayanarak yan覺tla. "
            "Eer balamda bilgi yoksa 'Elimdeki dok羹manlarda bu konuyla ilgili spesifik bilgi bulunmamaktad覺r.' diye cevap ver. "
            "Cevab覺n覺 k覺sa ve 繹z tut. Cevab覺n sonunda, kullan覺lan kayna覺 '[Kaynak: Dosya Ad覺, Sayfa X]' format覺nda belirt."
        )
        full_prompt = f"{system_prompt}\n\nBalam:\n{context}\n\nSoru: {question}\n\nCevap:"
        
        with st.spinner("Cevap oluturuluyor..."): 
            response = llm.generate_content(full_prompt)
        
        # 5. Kaynak bilgisini haz覺rla
        source_info = []
        if retrieved_metadatas:
            for meta in retrieved_metadatas:
                if meta: 
                    source, page = meta.get('source', '?'), meta.get('page', None)
                    source_info.append(f"{source}, Sayfa {page + 1}" if page is not None else source)
        unique_source_info = sorted(list(set(source_info)))
        
        # 6. Cevab覺 al ve Hata/G羹venlik durumunu ele al
        try: 
            final_answer = response.text
        except ValueError as e:
             final_answer = f"Model uygun cevap 羹retemedi. Detay: {repr(e)}"
             try:
                 if hasattr(response, 'prompt_feedback') and response.prompt_feedback:
                      final_answer += f" (Sebep: {response.prompt_feedback.block_reason.name if response.prompt_feedback.block_reason else 'Bilinmiyor'})"
             except AttributeError: pass
             
        # 7. Kayna覺 cevaba ekle
        if unique_source_info and not any(src.split(',')[0] in final_answer for src in unique_source_info): 
            final_answer += f" [Kaynak: {'; '.join(unique_source_info)}]"
        
        return final_answer
        
    except APIError as e: 
        return f"API HATA: Gemini servisine eriim salanamad覺. Detay: {repr(e)}"
    except Exception as e: 
        st.error(f"RAG Sorgulama Hatas覺: {e}"); st.error(traceback.format_exc()); 
        return f"GENEL HATA: RAG sorgusu ilenirken bir sorun olutu."

# =================================================================================
# 5. STREAMLIT ANA FONKS襤YON
# =================================================================================

def main():
    st.set_page_config(page_title="AI Ethics RAG Assistant", layout="wide")
    st.title(" AI Ethics & Compliance RAG Assistant")
    st.markdown("Yapay Zeka Etik ve Uyum Dok羹manlar覺na Dayal覺 Soru-Cevap Asistan覺")
    st.caption("Not: Bu uygulama Google Gemini ve ChromaDB kullanmaktad覺r.")

    # Bileenleri y羹kle (Cache ile)
    try:
        llm, embedding_function, text_splitter, collection = setup_rag_components()
    except Exception as e:
        st.stop() 

    # Bileenlerin baar覺yla y羹klenip y羹klenmediini kontrol et
    if not llm or not embedding_function or not text_splitter or not collection:
          st.error("Bileenlerden biri veya birka癟覺 y羹klenemedi. L羹tfen loglar覺 ve API key ayarlar覺n覺 kontrol edin.")
          st.stop()

    # Sidebar
    with st.sidebar:
        st.header("1. Dok羹man Y羹kleme (PDF)")
        uploaded_files = st.file_uploader("AI Etik ve Uyum PDF'lerini y羹kleyin", type="pdf", accept_multiple_files=True, key="file_uploader")

        if st.button("Dok羹manlar覺 襤le ve Kaydet"):
            if uploaded_files:
                try:
                    existing_ids = collection.get(include=[])['ids']
                    if existing_ids: collection.delete(ids=existing_ids); st.info("Mevcut veritaban覺 temizlendi.")
                except Exception as e: pass
                
                processed_count, chunk_count = index_documents(uploaded_files, collection, text_splitter, embedding_function)
                
                if chunk_count > 0:
                    st.success(f"Baar覺yla {processed_count}/{len(uploaded_files)} dosya ilendi ve {chunk_count} par癟a kaydedildi.")
                
                st.rerun() 
            else:
                st.warning("L羹tfen ilem yapmak i癟in bir PDF dosyas覺 y羹kleyin.")

        # Mevcut Kay覺t Say覺s覺
        try:
            doc_count = collection.count()
            st.info(f"Vekt繹r Veritaban覺nda Kay覺tl覺 Par癟a: {doc_count}")
        except Exception as e:
            doc_count = 0; st.info(f"Vekt繹r Veritaban覺nda Kay覺tl覺 Par癟a: {doc_count}")

    # Chat Aray羹z羹
    if "messages" not in st.session_state: 
        st.session_state.messages = [{"role": "assistant", "content": "Merhaba! L羹tfen sol panelden PDF'lerinizi y羹kleyip ileyin ve sohbeti balat覺n."}]
    
    for msg in st.session_state.messages: 
        st.chat_message(msg["role"]).write(msg["content"])
        
    if prompt := st.chat_input("Sorunuzu buraya yaz覺n..."):
        st.session_state.messages.append({"role": "user", "content": prompt})
        st.chat_message("user").write(prompt)
        
        try: current_doc_count = collection.count()
        except Exception: current_doc_count = 0
        
        if current_doc_count == 0: 
            response = "Veritaban覺nda ilenmi dok羹man yok. L羹tfen 繹nce dok羹man y羹kleyin ve 'Dok羹manlar覺 襤le ve Kaydet' butonuna bas覺n."
            st.chat_message("assistant").warning(response) 
        else:
            with st.chat_message("assistant"): 
                response = ask_rag_assistant(prompt, llm, collection, embedding_function)
                st.write(response)
                
        st.session_state.messages.append({"role": "assistant", "content": response})

if __name__ == "__main__":
    main()
