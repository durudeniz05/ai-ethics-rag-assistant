# -*- coding: utf-8 -*-

# =================================================================================
# 5. ADIM: STREAMLIT WEB UYGULAMASI (SON VE HATASIZ SRM)
# =================================================================================

import streamlit as st
import os
import glob
import tempfile
import textwrap

# RAG Bileenleri
from google import genai
from google.genai.errors import APIError
from chromadb import Client, Settings
from langchain_google_genai import GoogleGenerativeAIEmbeddings
from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain_community.document_loaders import PyPDFLoader


# --- 1. API Anahtar覺n覺 ve Bileenleri Haz覺rlama ---

# API Anahtar覺n覺 Streamlit Secrets'tan al覺yoruz
try:
    GEMINI_API_KEY = st.secrets["GEMINI_API_KEY"]
except KeyError:
    # Bu hata oluursa, uygulamay覺 durdurur ve kullan覺c覺ya mesaj g繹sterir
    st.error("HATA: Streamlit Secrets'ta 'GEMINI_API_KEY' bulunamad覺. L羹tfen kontrol edin.")
    st.stop()


@st.cache_resource
def setup_rag_components():
    """Gemini Client, Embedding Modeli, Text Splitter ve Chroma Collection'覺 haz覺rlar."""
    
    # 1. API Balant覺lar覺n覺 Hata Yakalama 襤癟inde Balatma
    try:
        # Gemini Client (LLM)
        gemini_client = genai.Client(api_key=GEMINI_API_KEY)
        
        # Embedding Modeli (Vekt繹rletirme)
        embedding_model_name = "text-embedding-004"
        embedding_function = GoogleGenerativeAIEmbeddings(
            model=embedding_model_name,
            api_key=GEMINI_API_KEY
        )
    except Exception as e:
        # API balant覺s覺nda bir hata olursa, ekran覺 癟繹kertmek yerine mesaj g繹sterir
        st.error(f"KR襤T襤K HATA: Gemini API Balant覺 Sorunu. Anahtar覺n覺z覺 ve Streamlit Secrets'覺 kontrol edin. Detay: {e}")
        st.stop()
        
    # 2. Metin Par癟alay覺c覺
    text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=100)
    
    # 3. Chroma Client (Vekt繹r DB)
    chroma_client = Client(Settings(allow_reset=True))
    collection_name = "ai_ethics_manual_collection"
    
    # Collection'覺 temizle ve yeniden olutur
    try:
        chroma_client.delete_collection(name=collection_name)
    except:
        pass 
        
    collection = chroma_client.get_or_create_collection(name=collection_name)
        
    return gemini_client, embedding_function, text_splitter, collection


# --- 2. Veri 襤leme ve Vekt繹r Kaydetme Fonksiyonu ---

def index_documents(uploaded_files, collection, text_splitter, embedding_function):
    """Y羹klenen dosyalar覺 iler ve Vekt繹r Veritaban覺'na kaydeder."""
    
    chunked_texts = []
    chunk_metadatas = []
    
    with tempfile.TemporaryDirectory() as temp_dir:
        for uploaded_file in uploaded_files:
            temp_file_path = os.path.join(temp_dir, uploaded_file.name)
            with open(temp_file_path, "wb") as f:
                f.write(uploaded_file.get_buffer())
            
            loader = PyPDFLoader(temp_file_path)
            documents = loader.load()
            chunks = text_splitter.split_documents(documents)

            for chunk in chunks:
                chunked_texts.append(chunk.page_content)
                chunk_metadatas.append({"source": uploaded_file.name})

    if not chunked_texts:
        st.error("Y羹klenen dosyalardan metin 癟覺kar覺lamad覺.")
        return 0

    # 4. Par癟alar覺 Embedding'e evirme
    try:
        with st.spinner("Dok羹manlar ileniyor ve vekt繹rlere 癟evriliyor..."):
            embeddings = embedding_function.embed_documents(chunked_texts) 
    except Exception as e:
        st.error(f"Embedding Hatas覺: Vekt繹r oluturulurken API'ye eriim salanamad覺. Detay: {e}")
        return 0

    # 5. Chroma'ya Kaydetme (Vekt繹rleri de ekleyerek)
    ids = [f"doc_{i}" for i in range(len(chunked_texts))]
    
    collection.add(
        documents=chunked_texts,
        embeddings=embeddings, 
        metadatas=chunk_metadatas,
        ids=ids
    )
    return len(chunked_texts)


# --- 3. RAG Sorgulama Fonksiyonu ---

def ask_rag_assistant(question, gemini_client, collection, embedding_function):
    """RAG sorgusunu 癟al覺t覺r覺r ve cevab覺 d繹nd羹r羹r."""
    try:
        # A. Sorguyu Vekt繹rletirme
        query_vector = embedding_function.embed_query(question)
        
        # B. Retrieval (Geri Getirme): Chroma DB'de arama
        results = collection.query(
            query_embeddings=[query_vector],
            n_results=3  # En alakal覺 3 par癟ay覺 getir
        )
        
        retrieved_chunks = results['documents'][0]
        retrieved_metadatas = results['metadatas'][0]
        
        # C. Generation (Cevap retme)
        context = "\n---\n".join(retrieved_chunks)
        system_prompt = (
            "Sen bir Yapay Zeka Etii ve Uyum asistan覺s覺n. Yaln覺zca salanan balamdaki bilgilere dayanarak yan覺tla. "
            "Eer balamda bilgi yoksa 'Elimdeki dok羹manlarda bu konuyla ilgili spesifik bilgi bulunmamaktad覺r.' diye cevap ver. "
            "Cevab覺n覺 k覺sa ve 繹z tut. Cevab覺n sonunda, kullan覺lan kayna覺 '[Kaynak: Dosya Ad覺]' format覺nda belirt."
        )
        
        full_prompt = f"{system_prompt}\n\nBalam: {context}\n\nSoru: {question}\n\nCevap:"
        
        response = gemini_client.models.generate_content(
            model='gemini-2.5-flash',
            contents=full_prompt
        )

        source_files = list(set([m['source'] for m in retrieved_metadatas]))
        final_answer = response.text
        
        if not any(source in final_answer for source in source_files):
             final_answer += f" [Kaynak: {', '.join(source_files)}]"

        return final_answer
        
    except APIError as e:
        return f"API HATA: Gemini servisine eriim salanamad覺. Detay: {e}"
    except Exception as e:
        return f"GENEL HATA: {e}"


# =================================================================================
# 4. STREAMLIT ANA FONKS襤YON
# =================================================================================

def main():
    st.set_page_config(page_title="AI Ethics & Compliance RAG Assistant", layout="wide")

    st.title(" AI Ethics & Compliance RAG Assistant")
    st.markdown("Yapay Zeka Etik ve Uyum Dok羹manlar覺na Dayal覺 Soru-Cevap Asistan覺")
    st.caption("Not: Bu uygulama, API hatalar覺n覺 amak i癟in manuel RAG kurulumu kullanmaktad覺r.")

    # RAG bileenlerini y羹kle
    gemini_client, embedding_function, text_splitter, collection = setup_rag_components()

    # --- Sol Panelde Dosya Y羹kleme ---
    with st.sidebar:
        st.header("1. Dok羹man Y羹kleme (PDF)")
        
        uploaded_files = st.file_uploader(
            "AI Etik ve Uyum PDF'lerini y羹kleyin", 
            type="pdf", 
            accept_multiple_files=True
        )

        if st.button("Dok羹manlar覺 襤le ve Kaydet"):
            if uploaded_files:
                # KR襤T襤K DZELTME: CHROMA S襤LME HATASI G襤DER襤LD襤
                # Bu komut, koleksiyondaki her eyi siler ve yeni y羹klemeye yer a癟ar.
                collection.delete(where={
                    "$and": [
                        {"source": {"$ne": "non_existent_source"}}
                    ]
                }) 
                
                chunk_count = index_documents(uploaded_files, collection, text_splitter, embedding_function)
                if chunk_count > 0:
                    st.success(f"Baar覺yla {len(uploaded_files)} dosya ilendi ve {chunk_count} par癟a kaydedildi.")
            else:
                st.warning("L羹tfen ilem yapmak i癟in bir PDF dosyas覺 y羹kleyin.")
                
        # Mevcut Kay覺t Say覺s覺
        st.info(f"Vekt繹r Veritaban覺nda Kay覺tl覺 Par癟a: {collection.count()}")

    # --- Ana Chat Aray羹z羹 ---
    if "messages" not in st.session_state:
        st.session_state["messages"] = [{"role": "assistant", "content": "Merhaba! L羹tfen sol panelden PDF'lerinizi y羹kleyip ileyin."}]

    for msg in st.session_state.messages:
        st.chat_message(msg["role"]).write(msg["content"])

    if prompt := st.chat_input("rn: AB Yapay Zeka Yasas覺'n覺n y羹ksek risk tan覺m覺 nedir?"):
        if collection.count() == 0:
            st.session_state.messages.append({"role": "user", "content": prompt})
            st.chat_message("user").write(prompt)
            st.chat_message("assistant").write("HATA: L羹tfen 繹nce dok羹manlar覺n覺z覺 y羹kleyin ve 'Dok羹manlar覺 襤le ve Kaydet' butonuna bas覺n.")
        else:
            st.session_state.messages.append({"role": "user", "content": prompt})
            st.chat_message("user").write(prompt)
            
            with st.chat_message("assistant"):
                with st.spinner("Asistan覺n覺z dok羹manlar覺 analiz ediyor..."):
                    response = ask_rag_assistant(prompt, gemini_client, collection, embedding_function)
                    st.session_state.messages.append({"role": "assistant", "content": response})
                    st.write(response)

if __name__ == "__main__":
    main()
